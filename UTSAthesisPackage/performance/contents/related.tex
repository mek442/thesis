
	\vspace{-0.2cm}
\section{Related Work}
\label{sec:related}
	\vspace{-0.1cm}
	
\textbf{Performance Testing and Faults.} Previous work focuses on generating performance test infrastructures and test cases, such as automated performance benchmarking~\cite{KALIBERA}, model-based performance testing framework for workloads~\cite{BARNA11}, using genetic algorithms to expose performance regressions~\cite{LUO16}, learning-based performance testing~\cite{GRECHANIK12}, symbolic-execution-based load-test generation~\cite{ZHANG11}, probabilistic symbolic execution~\cite{Chen2016}, and profiling-based test generation to reach performance bottlenecks~\cite{luoinput2016}. Pradel et al.~\cite{PradelISSTA2014} propose  an approach to support generation of multi-threaded tests based on single-threaded tests. Kwon et al.~\cite{ATC2013} propose an approach to predict execution time of a given input for Android apps. Bound analyses~\cite{SPEED} try to statically estimate the upper bound of loop iterations regarding input sizes, but they cannot be directly applied as the size of collection variables under a  certain test can be difficult to determine. Most recently, Padhye and Sen~\cite{PadhyeICSE2017} propose an  approach to identify collection traversals in program code; such approach has the potential to be used for execution-time prediction. In contrast to such previous work, our approach focuses on prioritizing existing performance test cases. The most related work in this direction is done by Huang et al.~\cite{huang2014performance}, whose differences with our approach are elaborated in Section~\ref{sec:intro}. 

Another related area is research on performance faults, including studies on performance faults~\cite{JIN12, PerfBugStudy}, static performance-fault detection \cite{Nistor14, JOVIC11, KILLIAN10, YAN12}, debugging of known performance faults \cite{SHEN05,HAN12,LEUNG07,AGUILERA03}, automatic patches of performance faults \cite{Nistor15}, and analysis of performance-testing results~\cite{FOO11,FOO10}. 


%The default approach for regression testing is to retest all test cases after releasing a new version, which is an expensive proposition. To solve this problem, there are good collection of industry case studies and research effort on performance regression testing in software systems. 

\noindent\textbf{Test Prioritization and Impact Analysis.} Test prioritization is a well explored area in regression testing to reduce test cost~\cite{HARROLD93,BLACK04,ZHONG06} or to detect functional faults earlier~\cite{ELBAUM00,KIM02,LI07}. Mocking~\cite{MockStudy} is another approach to reduce test cost, but it does not work for performance testing as mocked methods do not have normal execution time. Another related area is test selection or reduction~\cite{ROTHERMEL97,CHEN94,Hao2009} which sets a threshold or other criteria to select/remove part of the test cases. Most of the proposed efforts are based on some coverage criterion for test cases, and/or impact analysis of code commits. The impact analysis falls into three categories: static change impact analysis~\cite{TURVERref,Arnoldref96,Wang2010ASE}, dynamic impact analysis
~\cite{LAW03,ORSO11,APIWATTANAPON05}, and version-history-based impact analysis~\cite{ZIMMERMANN04,SHERRIFF08,MengHima}. Our approach leverages a similar strategy to rank performance tests according to the change impact on them. However, we propose specific techniques to estimate performance impacts, such as collection-loop correlation and performance impact analysis. 

%Functional regression testing is a well explored area to reduce testing cost by test case selection based on test case property andcode modification(), test suite reduction by removing redundancy in test suite () and test cases prioritization orders test case execution in a way to hope (). Different from these work, our goal is to reduce performance regression testing overhead via test suite prioritization based on change impact analysis whether an operation is expensive or lies in hot path. 

%\noindent\textbf{Impact Analysis.} The evolution of software systems and ongoing changes demand for explicit means to assess the impact of a change on existing artifacts and concepts. Thus, software change impact analysis is in the focus of researchers in software engineering. The important difference is that Our proposed method focus on the performance test suite prioritization  via performance impact implication of change.
